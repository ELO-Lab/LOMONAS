{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pickle as p\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FormatStrFormatter\n",
        "import pathlib\n",
        "from scipy import stats\n",
        "import matplotlib\n",
        "from scipy.stats import ks_2samp\n",
        "matplotlib.rcParams['pdf.fonttype'] = 42\n",
        "matplotlib.rcParams['ps.fonttype'] = 42\n",
        "plt.rc('legend', fontsize=18)\n",
        "plt.rc('xtick', labelsize=14)\n",
        "plt.rc('ytick', labelsize=14)\n",
        "plt.rc('axes', labelsize=22)"
      ],
      "metadata": {
        "id": "q5VY_XfDvY7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarize Results"
      ],
      "metadata": {
        "id": "_cUp48aQvUYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_res_evaluation_phase(list_variants):\n",
        "    for variant in list_variants:\n",
        "        path_file = f'{PATH_RESULTS}/{problem}/{problem}_{variant}'\n",
        "\n",
        "        nEvals_list = []\n",
        "        search_cost_list = []\n",
        "\n",
        "        EAs_list = []\n",
        "        IGD_list = []\n",
        "        IGDp_list = []\n",
        "        HV_list = []\n",
        "        best_arch_list = []\n",
        "        best_performance_list = []\n",
        "\n",
        "        for rid in range(31):\n",
        "            EAs_history = []\n",
        "            best_arch_history = []\n",
        "            best_performance_history = []\n",
        "\n",
        "            p_file_ = path_file + f'/{rid}'\n",
        "\n",
        "            try:\n",
        "                nEvals_history, EA_evaluate_history = p.load(open(p_file_ + '/#Evals_and_Elitist_Archive_evaluate.p', 'rb'))\n",
        "                _, IGD_history = p.load(open(p_file_ + '/#Evals_and_IGD.p', 'rb'))\n",
        "                _, IGDp_history = p.load(open(p_file_ + '/#Evals_and_IGDp.p', 'rb'))\n",
        "                _, HV_history = p.load(open(p_file_ + '/#Evals_and_HV.p', 'rb'))\n",
        "            except FileNotFoundError:\n",
        "                raise FileNotFoundError('Please download the prepared results!')\n",
        "            search_cost_history = p.load(open(p_file_ + '/running_time.p', 'rb'))\n",
        "\n",
        "            # Add the final evaluation_mark (i.e., 3000)\n",
        "            if nEvals_history[-1] != len(search_cost_history):\n",
        "                nEvals_history.append(len(search_cost_history))\n",
        "                EA_evaluate_history.append(EA_evaluate_history[-1])\n",
        "                IGD_history.append(IGD_history[-1])\n",
        "                IGDp_history.append(IGDp_history[-1])\n",
        "                HV_history.append(HV_history[-1])\n",
        "\n",
        "            nEvals_history = np.array(nEvals_history)\n",
        "            IGD_history = np.array(IGD_history)\n",
        "            IGDp_history = np.array(IGDp_history)\n",
        "            HV_history = np.array(HV_history)\n",
        "            search_cost_history = np.array(search_cost_history)\n",
        "\n",
        "            for EA in EA_evaluate_history:\n",
        "                F = np.round(EA['F'], 6)\n",
        "                ea = {\n",
        "                    'Approximation Set': EA['X'],\n",
        "                    'hashKey': EA['hashKey'],\n",
        "                    'Approximation Front': F\n",
        "                }\n",
        "                idx_best_arch = np.argmin(F[:, 0])\n",
        "                EAs_history.append(ea)\n",
        "                best_arch = EA['X'][idx_best_arch]\n",
        "                best_performance = np.round((1 - EA['F'][idx_best_arch][0]) * 100, 2)\n",
        "                best_arch_history.append(best_arch)\n",
        "                best_performance_history.append(best_performance)\n",
        "\n",
        "            new_nEvals_history = np.arange(20, 3001, 20)\n",
        "            new_IGD_history = []\n",
        "            new_IGDp_history = []\n",
        "            new_HV_history = []\n",
        "            new_EAs_history = []\n",
        "            new_best_arch_history = []\n",
        "            new_best_performance_history = []\n",
        "            for nEvals in new_nEvals_history:\n",
        "                idx = np.where(nEvals_history <= nEvals)[0][-1]\n",
        "                new_IGD_history.append(IGD_history[idx])\n",
        "                new_IGDp_history.append(IGDp_history[idx])\n",
        "                new_HV_history.append(HV_history[idx])\n",
        "                new_EAs_history.append(EAs_history[idx])\n",
        "                new_best_arch_history.append(best_arch_history[idx])\n",
        "                new_best_performance_history.append(best_performance_history[idx])\n",
        "            new_search_cost_history = search_cost_history[new_nEvals_history - 1]\n",
        "\n",
        "            nEvals_list.append(new_nEvals_history)\n",
        "            search_cost_list.append(new_search_cost_history)\n",
        "            EAs_list.append(new_EAs_history)\n",
        "            IGD_list.append(new_IGD_history)\n",
        "            IGDp_list.append(new_IGDp_history)\n",
        "            HV_list.append(new_HV_history)\n",
        "            best_arch_list.append(new_best_arch_history)\n",
        "            best_performance_list.append(new_best_performance_history)\n",
        "\n",
        "        nEvals_list = np.array(nEvals_list)\n",
        "        search_cost_list = np.array(search_cost_list)\n",
        "        IGD_list = np.array(IGD_list)\n",
        "        IGDp_list = np.array(IGDp_list)\n",
        "        HV_list = np.array(HV_list)\n",
        "        best_arch_list = np.array(best_arch_list)\n",
        "        best_performance_list = np.array(best_performance_list)\n",
        "\n",
        "        rs_ = {\n",
        "            'nEvals': nEvals_list,\n",
        "            'Search Cost': search_cost_list,\n",
        "            'EAs': EAs_list,\n",
        "            'IGD': IGD_list,\n",
        "            'IGD+': IGDp_list,\n",
        "            'HV': HV_list,\n",
        "            'Best Architecture': best_arch_list,\n",
        "            'Best Architecture (performance)': best_performance_list\n",
        "        }\n",
        "        p.dump(rs_, open(f'{PATH_RESULTS}/{problem}/{variant}_evaluation.p', 'wb'))\n",
        "\n",
        "def summarize_res_search_phase(list_variants):\n",
        "    for variant in list_variants:\n",
        "        path_file = f'{PATH_RESULTS}/{problem}/{problem}_{variant}'\n",
        "\n",
        "        nEvals_list = []\n",
        "        search_cost_list = []\n",
        "\n",
        "        EAs_list = []\n",
        "        IGD_list = []\n",
        "        IGDp_list = []\n",
        "        HV_list = []\n",
        "        best_arch_list = []\n",
        "        best_performance_list = []\n",
        "\n",
        "        for rid in range(31):\n",
        "            EAs_history = []\n",
        "            best_arch_history = []\n",
        "            best_performance_history = []\n",
        "\n",
        "            p_file_ = path_file + f'/{rid}'\n",
        "\n",
        "            try:\n",
        "                nEvals_history, EA_evaluate_history = p.load(open(p_file_ + '/#Evals_and_Elitist_Archive_search.p', 'rb'))\n",
        "                _, IGD_history = p.load(open(p_file_ + '/#Evals_and_IGD_search.p', 'rb'))\n",
        "                _, IGDp_history = p.load(open(p_file_ + '/#Evals_and_IGDp_search.p', 'rb'))\n",
        "                _, HV_history = p.load(open(p_file_ + '/#Evals_and_HV_search.p', 'rb'))\n",
        "            except FileNotFoundError:\n",
        "                raise FileNotFoundError('Please download the prepared results!')\n",
        "            search_cost_history = p.load(open(p_file_ + '/running_time.p', 'rb'))\n",
        "\n",
        "            # Add the final evaluation_mark (i.e., 3000)\n",
        "            if nEvals_history[-1] != len(search_cost_history):\n",
        "                nEvals_history.append(len(search_cost_history))\n",
        "                EA_evaluate_history.append(EA_evaluate_history[-1])\n",
        "                IGD_history.append(IGD_history[-1])\n",
        "                IGDp_history.append(IGDp_history[-1])\n",
        "                HV_history.append(HV_history[-1])\n",
        "\n",
        "            nEvals_history = np.array(nEvals_history)\n",
        "            IGD_history = np.array(IGD_history)\n",
        "            IGDp_history = np.array(IGDp_history)\n",
        "            HV_history = np.array(HV_history)\n",
        "            search_cost_history = np.array(search_cost_history)\n",
        "\n",
        "            for EA in EA_evaluate_history:\n",
        "                F = np.round(EA['F'], 6)\n",
        "                ea = {\n",
        "                    'Approximation Set': EA['X'],\n",
        "                    'hashKey': EA['hashKey'],\n",
        "                    'Approximation Front': F\n",
        "                }\n",
        "                idx_best_arch = np.argmin(F[:, 0])\n",
        "                EAs_history.append(ea)\n",
        "                best_arch = EA['X'][idx_best_arch]\n",
        "                best_performance = np.round((1 - EA['F'][idx_best_arch][0]) * 100, 2)\n",
        "                best_arch_history.append(best_arch)\n",
        "                best_performance_history.append(best_performance)\n",
        "\n",
        "            new_nEvals_history = np.arange(20, 3001, 20)\n",
        "            new_IGD_history = []\n",
        "            new_IGDp_history = []\n",
        "            new_HV_history = []\n",
        "            new_EAs_history = []\n",
        "            new_best_arch_history = []\n",
        "            new_best_performance_history = []\n",
        "            for nEvals in new_nEvals_history:\n",
        "                idx = np.where(nEvals_history <= nEvals)[0][-1]\n",
        "                new_IGD_history.append(IGD_history[idx])\n",
        "                new_IGDp_history.append(IGDp_history[idx])\n",
        "                new_HV_history.append(HV_history[idx])\n",
        "                new_EAs_history.append(EAs_history[idx])\n",
        "                new_best_arch_history.append(best_arch_history[idx])\n",
        "                new_best_performance_history.append(best_performance_history[idx])\n",
        "            new_search_cost_history = search_cost_history[new_nEvals_history - 1]\n",
        "\n",
        "            nEvals_list.append(new_nEvals_history)\n",
        "            search_cost_list.append(new_search_cost_history)\n",
        "            EAs_list.append(new_EAs_history)\n",
        "            IGD_list.append(new_IGD_history)\n",
        "            IGDp_list.append(new_IGDp_history)\n",
        "            HV_list.append(new_HV_history)\n",
        "            best_arch_list.append(new_best_arch_history)\n",
        "            best_performance_list.append(new_best_performance_history)\n",
        "\n",
        "        nEvals_list = np.array(nEvals_list)\n",
        "        search_cost_list = np.array(search_cost_list)\n",
        "        IGD_list = np.array(IGD_list)\n",
        "        IGDp_list = np.array(IGDp_list)\n",
        "        HV_list = np.array(HV_list)\n",
        "        best_arch_list = np.array(best_arch_list)\n",
        "        best_performance_list = np.array(best_performance_list)\n",
        "\n",
        "        rs_ = {\n",
        "            'nEvals': nEvals_list,\n",
        "            'Search Cost': search_cost_list,\n",
        "            'EAs': EAs_list,\n",
        "            'IGD': IGD_list,\n",
        "            'IGD+': IGDp_list,\n",
        "            'HV': HV_list,\n",
        "            'Best Architecture': best_arch_list,\n",
        "            'Best Architecture (performance)': best_performance_list\n",
        "        }\n",
        "        p.dump(rs_, open(f'{PATH_RESULTS}/{problem}/{variant}_search.p', 'wb'))"
      ],
      "metadata": {
        "id": "19-WTMr0vW1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "jFXdKLdct4u1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = {\n",
        "    'MOEA_NSGAII': 'NSGA-II',\n",
        "    'LOMONAS': 'LOMONAS (ours)',\n",
        "    'MOEA_MOEAD': 'MOEA/D',\n",
        "    'RR_LS': 'RR-LS',\n",
        "}\n",
        "\n",
        "color = {\n",
        "    'NSGA-II': ['blue', '--'],\n",
        "    'MOEA/D': ['green', '-.'],\n",
        "    'LOMONAS (ours)': ['tab:red', 'solid'],\n",
        "    'RR-LS': ['tab:orange', '--'],\n",
        "}\n",
        "\n",
        "def visualize_2D(ax, objective_0_mean, tmp_objective_1_mean, tmp_objective_1_stdev, label, line):\n",
        "    color = line[0]\n",
        "    style = line[1]\n",
        "    ax.plot(objective_0_mean, tmp_objective_1_mean, c=color, ls=style, label=label, linewidth=2)\n",
        "    ax.fill_between(objective_0_mean,\n",
        "                     tmp_objective_1_mean - tmp_objective_1_stdev,\n",
        "                     tmp_objective_1_mean + tmp_objective_1_stdev, alpha=0.1, fc=color)\n",
        "\n",
        "def visualize_results(ax, phase='evaluation', start_pt=20, end_pt=-1, metric='HV'):\n",
        "    performance_algo_list = {}\n",
        "    nEvals_algo_list = {}\n",
        "\n",
        "    for variant in variants_list:\n",
        "        results = p.load(open(f'{PATH_RESULTS}/{problem}/{variant}_{phase}.p', 'rb'))\n",
        "\n",
        "        nEvals_all = results['nEvals'][0]\n",
        "        idx_start = np.where(nEvals_all <= start_pt)[0][-1]\n",
        "\n",
        "        if end_pt == -1:\n",
        "            idx_end = None\n",
        "        else:\n",
        "            idx_end = np.where(nEvals_all <= end_pt)[0][-1]\n",
        "\n",
        "        if metric == 'HV':\n",
        "            perf = results['HV']\n",
        "        elif metric == 'IGD':\n",
        "            perf = results['IGD']\n",
        "        else:\n",
        "            perf = results['IGD+']\n",
        "\n",
        "        if idx_end is not None:\n",
        "            perf_mean, perf_std = np.mean(perf, axis=0)[idx_start:idx_end + 1], np.std(perf, axis=0)[idx_start:idx_end + 1]\n",
        "            nEvals = nEvals_all[idx_start:idx_end + 1]\n",
        "        else:\n",
        "            perf_mean, perf_std = np.mean(perf, axis=0), np.std(perf, axis=0)\n",
        "            nEvals = nEvals_all\n",
        "        performance_algo_list[variant] = [perf_mean, perf_std]\n",
        "        nEvals_algo_list[variant] = nEvals\n",
        "\n",
        "    perf_ind = performance_algo_list\n",
        "\n",
        "    ends = [100, 500, 1000, 3000]\n",
        "    for variant in variants_list:\n",
        "        visualize_2D(ax, nEvals_algo_list[variant], perf_ind[variant][0], perf_ind[variant][1], label=label[variant], line=color[label[variant]])\n",
        "    ax.set_xscale('log')\n",
        "    ax.set_xticks(ends)\n",
        "    ax.get_xaxis().set_major_formatter(FormatStrFormatter('%2d'))\n",
        "    ax.get_yaxis().set_major_formatter(FormatStrFormatter('%.3f'))\n",
        "\n",
        "    ax.set_ylabel(f'{title}')\n",
        "    ax.set_xlabel('#Evals')\n",
        "    ax.grid(True, linestyle='--')\n",
        "    ax.set_title(f'{metric} ({phase})', fontsize=24)\n",
        "    if metric == 'HV':\n",
        "        ax.legend(loc=4)\n",
        "    else:\n",
        "        ax.legend(loc=1)\n",
        "    plt.savefig(f'{PATH_RESULTS}/{problem}/{metric}_{phase}.jpg', bbox_inches='tight', pad_inches=0.1, dpi=300)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ltdoYib2rzVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## T-Test"
      ],
      "metadata": {
        "id": "xVG4hh4h0NVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def return_all_info(rs, comparison_point):\n",
        "    nEvals_all = rs['nEvals'][0]\n",
        "    if comparison_point == -1:\n",
        "        idx = -1\n",
        "    else:\n",
        "        idx = np.where(nEvals_all <= comparison_point)[0][-1]\n",
        "\n",
        "    IGD_lst = rs['IGD'][:, idx]\n",
        "    IGDp_lst = rs['IGD+'][:, idx]\n",
        "    HV_lst = rs['HV'][:, idx]\n",
        "    running_time_lst = rs['Search Cost'][:, idx]\n",
        "    best_arch_lst = rs['Best Architecture (performance)'][:, idx]\n",
        "    return IGD_lst, IGDp_lst, HV_lst, running_time_lst, best_arch_lst\n",
        "\n",
        "def compare(x, y, name_x, name_y, alpha, metric='IGD'):\n",
        "    p_value = stats.ttest_ind(x, y)[-1]\n",
        "\n",
        "    print(metric, name_x, name_y)\n",
        "    print('p_value:', p_value)\n",
        "\n",
        "    if np.isnan(p_value):\n",
        "        p_value = 1\n",
        "\n",
        "    if p_value <= alpha:\n",
        "        mean_1 = np.mean(x)\n",
        "        mean_2 = np.mean(y)\n",
        "        std_1 = np.std(x)\n",
        "        std_2 = np.std(y)\n",
        "        cohen_d = (abs(mean_1 - mean_2)) / ((std_1 ** 2 + std_2 ** 2) / 2) ** (1 / 2)\n",
        "        if cohen_d >= 0.8:\n",
        "            effect_size = 'large'\n",
        "        elif cohen_d >= 0.5:\n",
        "            effect_size = 'medium'\n",
        "        elif cohen_d >= 0.2:\n",
        "            effect_size = 'small'\n",
        "        else:\n",
        "            effect_size = 'trivial'\n",
        "        rs = np.mean(y) - np.mean(x)\n",
        "        if metric in ['HV', 'best_arch_found']:\n",
        "            rs *= -1\n",
        "        if rs > 0:\n",
        "            rs_compare = 'worse'\n",
        "        else:\n",
        "            rs_compare = 'better'\n",
        "        print('Reject |', rs_compare, effect_size)\n",
        "    print()\n",
        "\n",
        "def print_mean_std(_all, round_pre=4, print_std=True):\n",
        "    _mean = np.mean(_all, axis=0)\n",
        "    _std = np.std(_all, axis=0)\n",
        "    if print_std:\n",
        "        print(f'{np.round(_mean, round_pre)} ({np.round(_std, round_pre)})')\n",
        "    else:\n",
        "        print(f'{np.round(_mean, round_pre)}')\n",
        "\n",
        "def print_interval_confidence(_all, round_pre=4):\n",
        "    _mean = np.mean(_all, axis=0)\n",
        "    _std = np.std(_all, axis=0)\n",
        "    tmp = 3.657 * _std/(len(_all) ** (1/2))\n",
        "    print(f'({np.round(_mean - tmp, round_pre)} {np.round(_mean + tmp, round_pre)}) | {np.round(_mean, round_pre)}')\n",
        "\n",
        "\n",
        "def cal_interval_confidence(variants_lst, phase):\n",
        "    alpha = 0.01\n",
        "    print(f'Alpha: {alpha/len(variants_lst)}')\n",
        "\n",
        "    for variant in variants_lst:\n",
        "        print(variant)\n",
        "        rs = p.load(open(f'{PATH_RESULTS}/{problem}/{variant}_{phase}.p', 'rb'))\n",
        "        igd, igdp, hv, rt, best_arch = return_all_info(rs, -1)\n",
        "\n",
        "        print_interval_confidence(igd)\n",
        "        print_interval_confidence(igdp)\n",
        "        print_interval_confidence(hv)\n",
        "\n",
        "def t_test(variants_lst, phase, comparison_point):\n",
        "    alpha = 0.01/len(variants_lst)\n",
        "    print(f'Alpha: {alpha}')\n",
        "    print(f'Evals: {comparison_point}\\n')\n",
        "    competitors_lst = []\n",
        "    for variant in variants_lst:\n",
        "        variant_ = variant + f'_{phase}'\n",
        "        if pathlib.Path(f'{PATH_RESULTS}/{problem}/{variant_}.p').exists():\n",
        "            print(variant)\n",
        "            rs = p.load(open(f'{PATH_RESULTS}/{problem}/{variant_}.p', 'rb'))\n",
        "            igd, igdp, hv, rt, best_arch = return_all_info(rs, comparison_point)\n",
        "            print('IGD: ', end='')\n",
        "            print_mean_std(igd)\n",
        "            print('IGD+: ', end='')\n",
        "            print_mean_std(igdp)\n",
        "            print('HV: ', end='')\n",
        "            print_mean_std(hv)\n",
        "            print('Running time (in seconds): ', end='')\n",
        "            print_mean_std(rt, 0, False)\n",
        "            print('Best Architecture: ', end='')\n",
        "            print_mean_std(best_arch, 2)\n",
        "\n",
        "            performance = {\n",
        "                'variant': variant,\n",
        "                'IGD': igd,\n",
        "                'IGD+': igdp,\n",
        "                'HV': hv,\n",
        "                'best_arch_found': best_arch\n",
        "            }\n",
        "            competitors_lst.append(performance)\n",
        "            print('-' * 10)\n",
        "    print('='*20)\n",
        "    print()\n",
        "    for metric in ['IGD', 'IGD+', 'HV']:\n",
        "        print('Metric:', metric)\n",
        "        for i in range(len(competitors_lst)):\n",
        "            for j in range(i + 1, len(competitors_lst)):\n",
        "                pair = [competitors_lst[i][metric], competitors_lst[j][metric],\n",
        "                        competitors_lst[i]['variant'], competitors_lst[j]['variant'], alpha, metric]\n",
        "                compare(*pair)\n",
        "        print('='*20)\n",
        "        print()"
      ],
      "metadata": {
        "id": "mkbneS5L0MlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "FosZVQdoxBTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarize results and Visualize"
      ],
      "metadata": {
        "id": "BYc_FNq20Yl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_RESULTS = '/content/LOMONAS/exp_res'  # Your experiment result path\n",
        "problem = 'NAS201-C10'\n",
        "\n",
        "print(problem)\n",
        "if problem == 'NAS201-C10':\n",
        "    title = 'NAS-201 (CIFAR-10)'\n",
        "elif problem == 'NAS201-C100':\n",
        "    title = 'NAS-201 (CIFAR-100)'\n",
        "elif problem == 'NAS201-IN16':\n",
        "    title = 'NAS-201 (IN16-120)'\n",
        "elif problem == 'NAS101':\n",
        "    title = 'NAS-Bench-101'\n",
        "elif problem == 'MacroNAS-C10':\n",
        "    title = 'MacroNAS (CIFAR-10)'\n",
        "elif problem == 'NAS-ASR':\n",
        "    title = 'NAS-Bench-ASR'\n",
        "else:\n",
        "    title = 'MacroNAS (CIFAR-100)'\n",
        "\n",
        "################################################ Summarize Results #################################################\n",
        "variants_list = [\n",
        "    'MOEA_NSGAII',\n",
        "    'MOEA_MOEAD',\n",
        "    'RR_LS',\n",
        "    'LOMONAS'\n",
        "]\n",
        "\n",
        "# Summarize results\n",
        "summarize_res_search_phase(variants_list)\n",
        "summarize_res_evaluation_phase(variants_list)\n",
        "\n",
        "################################################ Visualize Results #################################################\n",
        "for phase in ['search', 'evaluation']:\n",
        "    for i, metric in enumerate(['IGD', 'IGD+', 'HV']):\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        visualize_results(ax=ax, phase=phase, start_pt=100, end_pt=3000, metric=metric)"
      ],
      "metadata": {
        "id": "vmxU0tQCxCOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run statitical tests"
      ],
      "metadata": {
        "id": "pLiqZzQ40I5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for phase in ['search', 'evaluation']:\n",
        "    print('Phase:', phase.upper())\n",
        "    variants_list = [\n",
        "        'MOEA_NSGAII',\n",
        "        'MOEA_MOEAD',\n",
        "        'RR_LS',\n",
        "        'LOMONAS'\n",
        "    ]\n",
        "\n",
        "    t_test(variants_list, phase=phase, comparison_point=3000)"
      ],
      "metadata": {
        "id": "UcfdSDTF0Jv6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}